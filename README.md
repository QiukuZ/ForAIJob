## 面试内容查漏补缺

### 1.传统几何相关SLAM

##### 1.1旋转矩阵和四元数的关系？

旋转矩阵用9自由度描述三自由度旋转，冗余；旋转向量和欧拉角是紧凑的，但是有奇异性。

四元数是一种紧凑+非奇异表达。**三维旋转可以用单位四元数表示。**

##### 1.2单应矩阵的作用&求解最少需要点书？

##### 1.3基础矩阵&本质矩阵

##### 1.4畸变类型

##### 1.5Ransac

RANSAC全称：随机采样一致算法。

##### 1.6高斯牛顿&LM

##### 1.7有哪些鲁棒核函数

##### 1.8激光雷达的畸变如何产生

##### 1.9什么是边缘化

##### 1.10什么是ORB特征，ORB特征的旋转不变性是如何做的

### 2.深度学习相关

##### 2.1梯度消失&梯度爆炸

##### 2.2BatchNorm

因为深层神经网络在做非线性变换前的激活输入值（就是那个x=WU+B，U是输入）随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢,所以这导致反向传播时低层神经网络的梯度消失，这是训练深层神经网络收敛越来越慢的.

BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布 ，其实就是把越来越偏的分布强制拉回比较标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化,**记住“保持数据同分布”即可**。

##### 2.3BatchNorm作用

![image-20220725195855937](/Users/qk/Library/Application Support/typora-user-images/image-20220725195855937.png)

##### 2.4常用的Loss项

##### 2.5几种常用的优化器

SGD：是一种使用梯度去迭代更新权重参数使目标函数最小化的方法。

mini-batch：小批量梯度下降算法是折中方案，选取训练集中一个小批量样本（一般是2的倍数，如32，64,128等）计算，这样可以保证训练过程更稳定，而且采用批量训练方法也可以利用矩阵计算的优势。这是目前最常用的梯度下降算法。

随机梯度下降：随机梯度下降算法是另外一个极端，损失函数是针对训练集中的一个训练样本计算的，又称为在线学习，即得到了一个样本，就可以执行一次参数更新。所以其收敛速度会快一些，但是有可能出现目标函数值震荡现象，因为高频率的参数更新导致了高方差。

动量梯度下降：引入一个指数加权平均的知识点。

RMSprop：更新权重的时候，使用除根号的方法，可以使较大的梯度大幅度变小，而较小的梯度小幅度变小，这样就可以使较大梯度方向上的波动小下来，那么整个梯度下降的过程中摆动就会比较小

Adom：Adam算法结合了Momentum和RMSprop梯度下降法，是一种极其常见的学习算法，被证明能有效适用于不同神经网络，适用于广泛的结构。

##### 2.6常用的激活函数

sigmoid、ReLU、Tanh

